{"cells":[{"cell_type":"code","execution_count":null,"id":"zMqFk6eBOxpD","metadata":{"id":"zMqFk6eBOxpD"},"outputs":[],"source":["!git clone https://git.wur.nl/koots006/msc-course-machine-learning.git data\n","!mv data/modules/regsubset.py ."]},{"cell_type":"markdown","id":"809e96a0","metadata":{"id":"809e96a0","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["# 6.5 Lab: Linear Models and Regularization Methods\n","\n","**First**: Read Chapter 6.1, 6.2.\n","\n","This lab has the following sections:\n","* Best Subset Selection Methods\n","* Forward and Backward Stepwise Selection\n","* Choosing Among Models Using The Validation-Set Approach and Cross-Validation\n","* Ridge Regression\n","* The Lasso Regression"]},{"cell_type":"code","execution_count":null,"id":"bd3d4e7b","metadata":{"id":"bd3d4e7b"},"outputs":[],"source":["# These are added for the regsubset implementation\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from regsubset import OLS, exhaustive_search, forward_search, backward_search\n","from sklearn.metrics import mean_squared_error"]},{"cell_type":"markdown","id":"2439aa84","metadata":{"id":"2439aa84"},"source":["## 6.5.1 Subset Selection Methods\n","### Best Subset Selection\n","Here we apply the best subset selection approach to the `Hitters` data.\n","We wish to predict a baseball player's `Salary` on the basis of various statistics associated with performance in the previous year. First, let's load the data:"]},{"cell_type":"code","execution_count":null,"id":"15a5f912","metadata":{"id":"15a5f912"},"outputs":[],"source":["Hitters = pd.read_csv('./data/islr_data/Hitters.csv')\n","Hitters.head()"]},{"cell_type":"markdown","id":"1Cgk5jlVHRmF","metadata":{"id":"1Cgk5jlVHRmF"},"source":["You can see above that the `Salary` variable is missing for some of the players.  The `isna()` function can be used to identify the missing observations. It returns a vector of the same length as the input vector, with a `True` for any elements that are missing, and a `False` for non-missing elements. The `sum()` function can then be used to count all of the missing elements."]},{"cell_type":"code","execution_count":null,"id":"92c68df4","metadata":{"id":"92c68df4"},"outputs":[],"source":["print(\"Shape: \", Hitters.shape)\n","print(\"Nr of NA values in `Salary`:\", sum(pd.isna(Hitters['Salary'])))"]},{"cell_type":"markdown","id":"b80b143e","metadata":{"id":"b80b143e"},"source":["Hence we see that `Salary` is missing for $59$ players. The `.dropna()` function removes all of the rows that have missing values in any variable."]},{"cell_type":"code","execution_count":null,"id":"b8a84d09","metadata":{"id":"b8a84d09"},"outputs":[],"source":["Hitters = Hitters.dropna()\n","print(\"Shape: \", Hitters.shape)\n","print(\"Nr of na values in `Salary`:\", Hitters[\"Salary\"].isnull().sum())"]},{"cell_type":"markdown","id":"muia9ZFzH__b","metadata":{"id":"muia9ZFzH__b"},"source":["Next, we remove the names and convert the three categorical variables to dummies, binary variables indicating each category (1-0 or 0-1). As these variables are binary, we actually can make do with one dummy variable (1 or 0). Finally, we split the data into the predictors (X) and the response (y)."]},{"cell_type":"code","source":["# We need to convert our categorical predicors into dummies\n","Hitters = pd.get_dummies(Hitters, columns=['League', 'Division', 'NewLeague'])"],"metadata":{"id":"OwjpjELqInNN"},"id":"OwjpjELqInNN","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Exercise:**\n","  - Check Hitters dataframe, which columns were added after using the get_dummies function?"],"metadata":{"id":"p-7p8CaKIwxl"},"id":"p-7p8CaKIwxl"},{"cell_type":"code","source":["#ToDo\n","...?"],"metadata":{"id":"ggWJeInkJp6B"},"id":"ggWJeInkJp6B","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"7bbcc4a1","metadata":{"id":"7bbcc4a1"},"outputs":[],"source":["# Each categorical feature only has two categories, so we can delete the second column for every dummy\n","Hitters = Hitters.drop('League_A', axis=1).drop('Division_E', axis=1).drop('NewLeague_A', axis=1)\n","\n","y = Hitters['Salary']              # We want to predict `Salary` with all other predictors\n","X = Hitters.drop('Salary', axis=1) # We want to use all predictors except `Salary`, of course"]},{"cell_type":"markdown","id":"be820dd9","metadata":{"id":"be820dd9"},"source":["The book uses an R function `regsubsets()` which performs subset selection for linear regression. An identical function is not available in Python, so we provide some pre-made code in the accompanying `regsubset.py` module ourselves. At the end of this block we will also show how you can work with subset selection in scikit-learn.\n","\n","`exhaustive_search()` performs best subset selection by identifying the best model that contains a given number of predictors, where *best* is quantified using RSS. If you want, you can have a look at the code in the `regsubset.py` module; you may use it to learn how things are implemented, but do not worry if you do not immediately understand things, as you will later use pre-defined functions. Note that `regsubset.py` provides a function OLS, which is a wrapper for the same function in `statsmodels`, adapted for feature subset selection."]},{"cell_type":"markdown","id":"e44d1443","metadata":{"id":"e44d1443"},"source":["We first instantiate our model wrapper from the regsubset module. `fit_intercept = True` means that we want to also fit an intercept."]},{"cell_type":"code","execution_count":null,"id":"f6ed55e3","metadata":{"id":"f6ed55e3"},"outputs":[],"source":["lm = OLS(fit_intercept=True)"]},{"cell_type":"markdown","id":"e3b8d966","metadata":{"id":"e3b8d966"},"source":["Then we can supply wrapped model to our subset search function. The `nvmax` parameter determines the maximum size of our feature subset. For example, `nvmax = 4` means that the search function will look for the best model with 1 feature, the best model with 2 features, the best model with 3 features, and the best model with 4 features. Let's try to go up to 4 features:"]},{"cell_type":"markdown","source":["**Exercise**\n"," - Add the missing parameter and try with 4 and 7 features."],"metadata":{"id":"3wJuLy8AMYhK"},"id":"3wJuLy8AMYhK"},{"cell_type":"code","execution_count":null,"id":"a87134e2","metadata":{"id":"a87134e2"},"outputs":[],"source":["#ToDo\n","lm_exhaustive = exhaustive_search(lm, X, y, ...?)"]},{"cell_type":"markdown","id":"bb97b8ca","metadata":{"id":"bb97b8ca"},"source":["With the `exhaustive_search()` function we have found the best models trained with 1 feature up to 7 features (all with an intercept); if you are brave, you can try increasing `nvmax`, but beware the time needed will increase quickly.\n","\n","Have a look at which features were picked by the algorithm. What do you notice?"]},{"cell_type":"markdown","source":["**Exercise**\n"," - Have a look at which features were picked by the algorithm. What do you notice? Hint: You can get the models with lm_exhaustive.models, for each model you can use model.coefs() function which returns two values: name (variable) and coef."],"metadata":{"id":"_cnV9oT0NVUp"},"id":"_cnV9oT0NVUp"},{"cell_type":"code","execution_count":null,"id":"02417bf1","metadata":{"id":"02417bf1"},"outputs":[],"source":["#ToDo\n","...?"]},{"cell_type":"markdown","id":"oGmGfsw3XQgN","metadata":{"id":"oGmGfsw3XQgN"},"source":["The method also stores the metrics for each best subset model, which match those in ISLR (note that $C_p$ and $AIC$ are proportional for least squares regression), in a dictionary `metrics`. Below, we convert this to a dataframe only for ease of printing:"]},{"cell_type":"code","execution_count":null,"id":"GtXLEukTXYuP","metadata":{"id":"GtXLEukTXYuP"},"outputs":[],"source":["pd.DataFrame.from_dict(lm_exhaustive.metrics)"]},{"cell_type":"markdown","id":"bf98aee5","metadata":{"id":"bf98aee5"},"source":["The exhaustive search function outputs a `Result` object. This `Result` object has a method `best_model()` for returning the best model based on a given metric. Here we want to know what the best model is (i.e., what the best feature subset is) based on $R_2^{adjusted}$. The best model has the highest $R_2^{adjusted}$. How many features are optimal?"]},{"cell_type":"code","execution_count":null,"id":"5b2866d0","metadata":{"id":"5b2866d0"},"outputs":[],"source":["lm_exhaustive.best_model(metric='rsquared_adj', best=max)"]},{"cell_type":"markdown","id":"c765ebd9","metadata":{"id":"c765ebd9"},"source":["We can also select a specific other model included in the results, for example the best model trained on a subset of a certain number of features, by specifying the parameter `n`.\n","\n","**Exercise**\n"," - Use the code to select the model with 6 features."]},{"cell_type":"code","execution_count":null,"id":"37c7162b","metadata":{"id":"37c7162b"},"outputs":[],"source":["#ToDo\n","lm_exhaustive.get_model(n=...?)"]},{"cell_type":"markdown","id":"e86c19b8","metadata":{"id":"e86c19b8"},"source":["We can also visualize the metrics calculated to facilitate choosing an optimal subset, as follows. According to which metric is a subset of 6 features optimal?"]},{"cell_type":"code","execution_count":null,"id":"7b8b8d08","metadata":{"id":"7b8b8d08"},"outputs":[],"source":["lm_exhaustive.plot()"]},{"cell_type":"markdown","id":"b0c80609","metadata":{"id":"b0c80609"},"source":["### Forward and Backward Stepwise Selection\n","Next to exhaustive search, we can also use the `forward_search()` for forward search and `backward_search()` for backward search. Instead of `nvmax`, `backward_search()` takes in `nvmin`, since the search for best subset per feature subset size is backwards."]},{"cell_type":"markdown","id":"c69a166f","metadata":{"id":"c69a166f"},"source":["For instance, we see that using forward stepwise selection, the best one-variable model contains only `CRBI`, and the best two-variable model additionally includes `Hits`:\n"]},{"cell_type":"code","execution_count":null,"id":"230f68e3","metadata":{"id":"230f68e3"},"outputs":[],"source":["lm = OLS(fit_intercept=True)\n","lm_forward = forward_search(lm, X, y, nvmax=19)"]},{"cell_type":"markdown","source":["**Exercise**\n","- Obtain the best models for 1 and 2 features using forward search."],"metadata":{"id":"2KQu1F57Clw2"},"id":"2KQu1F57Clw2"},{"cell_type":"code","execution_count":null,"id":"9b8d5340","metadata":{"id":"9b8d5340"},"outputs":[],"source":["#ToDo\n","lm_forward.get_model(n=...?)"]},{"cell_type":"code","execution_count":null,"id":"a0d582ea","metadata":{"id":"a0d582ea"},"outputs":[],"source":["#ToDo\n","lm_forward.get_model(n=...?)"]},{"cell_type":"markdown","id":"c18cd4d9","metadata":{"id":"c18cd4d9"},"source":["Did you notice how much faster forward search is than exhaustive search?\n","\n","For this data, the best one-variable through six-variable models are each identical for best subset and forward selection.\n","\n","**Exercise**\n"," - Print the best variables for models 1 to 6. You can get the models with lm_forward.models, for each model you can use model.coefs() function which returns two values: name (variable) and coef."]},{"cell_type":"code","execution_count":null,"id":"d4808c72","metadata":{"id":"d4808c72"},"outputs":[],"source":["#ToDo\n","...?"]},{"cell_type":"markdown","id":"0ad4cb28","metadata":{"id":"0ad4cb28"},"source":["However, the best seven-variable models identified by forward stepwise selection, backward stepwise selection, and best subset selection are different:"]},{"cell_type":"code","execution_count":null,"id":"50bd19fe","metadata":{"id":"50bd19fe"},"outputs":[],"source":["lm_exhaustive.get_model(n=7).coefs()"]},{"cell_type":"code","execution_count":null,"id":"2ff1b286","metadata":{"id":"2ff1b286"},"outputs":[],"source":["lm_forward.get_model(n=7).coefs()"]},{"cell_type":"code","execution_count":null,"id":"85e52e83","metadata":{"id":"85e52e83"},"outputs":[],"source":["lm = OLS(fit_intercept=True)\n","lm_backward = backward_search(lm, X, y, nvmin=1)\n","lm_backward.get_model(n=7).coefs()"]},{"cell_type":"markdown","id":"0b5c53c4","metadata":{"id":"0b5c53c4"},"source":["### Choosing Among Models Using the Validation-Set Approach and Cross-Validation\n","We just saw that it is possible to choose among a set of models of different sizes using Mallow's $C_{p}$, $BIC$, and $R^{2}_{adjusted}$. We will now consider how to do this using the validation set and cross-validation approaches.\n","\n","In order for these approaches to yield accurate estimates of the test error, we must use *only the training observations* to perform all aspects of model-fitting - including variable selection. Therefore, the determination of which model of a given size is best must be made using *only the training observations*. This point is subtle but important. If the full data set is used to perform the best subset selection step, the validation set errors and cross-validation errors that we obtain will not be accurate estimates of the test error.\n","\n","To use the validation set approach, we begin by splitting the observations into a training set and a test set:"]},{"cell_type":"code","execution_count":null,"id":"412124f2","metadata":{"id":"412124f2"},"outputs":[],"source":["np.random.seed(42)\n","mask = np.random.rand(len(Hitters)) < 0.5\n","train = Hitters[mask]\n","val = Hitters[~mask]\n","print(mask.shape, train.shape, val.shape)"]},{"cell_type":"markdown","id":"803874e2","metadata":{"id":"803874e2"},"source":["Now we apply `forward_search()` on the training set in order to perform best subset selection:"]},{"cell_type":"code","execution_count":null,"id":"071876d5","metadata":{"id":"071876d5"},"outputs":[],"source":["X_train = train.drop('Salary', axis=1)\n","y_train = train['Salary']\n","\n","lm = OLS(fit_intercept=True)\n","lm_val = forward_search(lm, X_train, y_train, nvmax=18)"]},{"cell_type":"markdown","id":"f4f6720f","metadata":{"id":"f4f6720f"},"source":["We can then use a small for-loop to calculate the error on the test set with each model.\n","\n","**Exercise**\n"," - In the following block of code, compute the $val\\_error$ (Mean Squared Error (MSE))."]},{"cell_type":"code","execution_count":null,"id":"98eed016","metadata":{"id":"98eed016"},"outputs":[],"source":["X_val = val.drop('Salary', axis=1)\n","y_val = val['Salary']\n","\n","val_errors = []\n","for n in lm_val.ns:\n","    model = lm_val.models[n - 1]\n","    subset = lm_val.subsets[n - 1]\n","    y_pred = model.predict(X_val, subset)\n","    val_error = ...?\n","    val_errors.append(val_error)"]},{"cell_type":"code","execution_count":null,"id":"afc73a47","metadata":{"id":"afc73a47","scrolled":true},"outputs":[],"source":["for i, val_error in enumerate(val_errors): print(i + 1, val_error)"]},{"cell_type":"code","execution_count":null,"id":"47b5855d","metadata":{"id":"47b5855d"},"outputs":[],"source":["best = val_errors.index(min(val_errors))\n","lm_val.get_model(best + 1)  # +1 since index 0 contains score for model with 1 feature"]},{"cell_type":"markdown","id":"8fa7594b","metadata":{"id":"8fa7594b"},"source":["According to the validation-set approach combined with forward-search, a 5-10 feature model is optimal.\n","\n","**Exercise**\n"," - Note that this result depends on the particular train/validation set split. Try changing the random seed above to see how stable this result is.\n","\n","Finally, to get good estimates of the coefficients, you should perform best subset selection on the full data set, and select the best model. It is important that we make use of the full dataset in order to obtain more accurate coefficient estimates. Note that we perform best subset selection on the full dataset and select the best model with the number of variables just found as optimal, rather than simply using the variables that were obtained from the training set, because the best variable subset model on the full data set may differ from the corresponding model on the training set."]},{"cell_type":"code","execution_count":null,"id":"9b4941ea","metadata":{"id":"9b4941ea"},"outputs":[],"source":["lm = OLS(fit_intercept=True)\n","lm_full = exhaustive_search(lm, X=Hitters.drop('Salary', axis=1), y=Hitters['Salary'], nvmax=best)\n","lm_full.get_model(n=best).coefs()"]},{"cell_type":"markdown","id":"2fbf599f","metadata":{"id":"2fbf599f"},"source":["In fact, we see that the best variable subset model on the full data set has a different set of variables than the best 7-variable model on the training set."]},{"cell_type":"markdown","id":"13e97131","metadata":{"id":"13e97131"},"source":["We will now try to choose among the models of different sizes using cross-validation. This approach is somewhat involved, as we must perform selection (here, forward selection) *within each of the k training sets*. We write a for-loop that performs cross-validation. In the *j*th fold, the elements of `kfolds` that equal `i` are in the test set, and the remainder are in the training set. We make our predictions for each model size and compute the test errors on the appropriate subset.  "]},{"cell_type":"code","execution_count":null,"id":"a5dc66e7","metadata":{"id":"a5dc66e7"},"outputs":[],"source":["def cross_val(data,labels,kfolds=10):\n","    data = data.to_numpy()\n","    labels = labels.to_numpy()\n","\n","    folds = np.array_split(range(len(data)), kfolds)\n","    val_errors = {} # Dictionary: k CV errors for each number of features 1..19\n","\n","    for i, fold in enumerate(folds):\n","        X_train = data[np.concatenate([fold for j, fold in enumerate(folds) if j != i])]\n","        y_train = labels[np.concatenate([fold for j, fold in enumerate(folds) if j != i])]\n","        X_val = data[folds[i]]\n","        y_val = labels[folds[i]]\n","\n","        lm = OLS(fit_intercept=True)\n","        lm_cv = forward_search(lm, X_train, y_train, nvmax=19)\n","\n","        for n in lm_cv.ns:\n","            model = lm_cv.models[n - 1]\n","            subset = lm_cv.subsets[n - 1]\n","            y_pred = model.predict(X_val, subset)\n","            val_error = mean_squared_error(y_val,y_pred) #((y_val - y_pred) ** 2).mean()\n","            # Add to dictionary\n","            if n-1 in val_errors:\n","                val_errors[n-1].append(val_error)\n","            else:\n","                val_errors[n-1] = [val_error]\n","\n","    return val_errors"]},{"cell_type":"markdown","source":["**Exercise**\n"," - Run a 10-fold cross-validation using the cross_val function defined above and store the results in $val\\_errors$ variable."],"metadata":{"id":"casV97fTUmWg"},"id":"casV97fTUmWg"},{"cell_type":"code","execution_count":null,"id":"30a5dddf","metadata":{"id":"30a5dddf"},"outputs":[],"source":["#ToDo\n","val_errors = cross_val(...? , ...?)"]},{"cell_type":"code","execution_count":null,"id":"fff1bd60","metadata":{"id":"fff1bd60"},"outputs":[],"source":["# Calculate average error over all folds for each number of features\n","errors = [sum(errors) / len(errors) for nfeat, errors in val_errors.items()]\n","\n","# Find number of features with lowest average CV error\n","best = errors.index(min(errors)) + 1  # +1 since index 0 contains score for model with 1 feature\n","\n","print(best)"]},{"cell_type":"markdown","id":"5a516800","metadata":{"id":"5a516800"},"source":["We see that cross-validation selects an 8-variable model. We now perform forward selection\n","on the full data set in order to obtain the 8-variable model. The cross validation results are a lot more representative than those of the single train/validation split:"]},{"cell_type":"code","execution_count":null,"id":"698669c3","metadata":{"id":"698669c3"},"outputs":[],"source":["lm = OLS(fit_intercept=True)\n","lm_forward = forward_search(lm, Hitters.drop('Salary', axis=1), Hitters['Salary'], nvmax=19)\n","lm_forward.get_model(n=best)"]},{"cell_type":"markdown","id":"Rvs7FyWDiRfo","metadata":{"id":"Rvs7FyWDiRfo"},"source":["### Subset selection in scikit-**learn**"]},{"cell_type":"markdown","id":"ZCbCbh4Xipzj","metadata":{"id":"ZCbCbh4Xipzj"},"source":["Above, you have worked with custom code that connects well to the ISLR explanation of subset selection. The scikit-learn package that we often use has a built-in feature selection method, which is less flexible but is easier to use with more complex classifiers later. Here is an example, using forward or backward best subset selection with 10-fold cross-validation to find 7 features:"]},{"cell_type":"code","execution_count":null,"id":"w12I9GtxktYB","metadata":{"id":"w12I9GtxktYB"},"outputs":[],"source":["from sklearn import linear_model\n","from sklearn.feature_selection import SequentialFeatureSelector\n","\n","reg_model = linear_model.LinearRegression()\n","\n","forward_sfs = SequentialFeatureSelector(reg_model,\n","                                        n_features_to_select=7,\n","                                        direction='forward',\n","                                        scoring='neg_mean_squared_error',\n","                                        cv=10).fit(X,y)\n","\n","backward_sfs = SequentialFeatureSelector(reg_model,\n","                                         n_features_to_select=7,\n","                                         direction='backward',\n","                                         scoring='neg_mean_squared_error',\n","                                         cv=10).fit(X,y)\n","\n","print(\"Forward selection\", forward_sfs.get_feature_names_out())\n","print(\"Backward selection\", backward_sfs.get_feature_names_out())"]},{"cell_type":"markdown","id":"5WsvDvdi_qYn","metadata":{"id":"5WsvDvdi_qYn"},"source":["Note that due to the cross-validation, you may get different results here than using the code above. To obtain a dataset that contains just the selected features, you can use the `get_support()` or `transform()` functions:"]},{"cell_type":"code","execution_count":null,"id":"9cIm-zKJAiOu","metadata":{"id":"9cIm-zKJAiOu"},"outputs":[],"source":["print(X.shape)\n","\n","X1 = X.loc[:,forward_sfs.get_support()]   # get_support() returns a boolean array, True for selected features\n","X2 = forward_sfs.transform(X)             # applies subset selection directly, returns a NumPy Array\n","\n","print(X1.head())\n","print(X2[0:5,:])"]},{"cell_type":"markdown","id":"ojjgsyn0DB41","metadata":{"id":"ojjgsyn0DB41"},"source":["An advantage of using the scikit-learn framework is that you can easily apply the same function for many models, including classifiers.\n","\n","**Exercise**\n"," - Use the code to predict the label `Division_W` using K-Neighbors classifier, forward search, and 7 features."]},{"cell_type":"code","execution_count":null,"id":"CTt50-VzC_lk","metadata":{"id":"CTt50-VzC_lk"},"outputs":[],"source":["#ToDo\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","X_class = Hitters.drop('Division_W', axis=1)\n","y_class = Hitters['Division_W']\n","\n","knn_model = KNeighborsClassifier(n_neighbors=3)\n","\n","forward_sfs = SequentialFeatureSelector(...?,\n","                                        n_features_to_select=...?,\n","                                        direction=...?,\n","                                        scoring='accuracy',\n","                                        cv=10).fit(X_class,y_class)\n","\n","print(forward_sfs.get_feature_names_out())"]},{"cell_type":"markdown","id":"af169bfc","metadata":{"id":"af169bfc"},"source":["### BONUS: DIY Subset Selection for Classification\n","**Note: dive into this only when you have time left and are comfortable reading and editing the search functions from the `regsubset` module.**\n","\n","1. Can you finish the scikit-learn classifier wrapper that can be used by the `exhaustive_search()` function? Try to write a wrapper that can take in a k-nearest neighbor model. You will need to select a new target variable from the Hitters dataset which can be used for classification, like `Division_W` above.\n","\n","Some code to get you started:"]},{"cell_type":"code","execution_count":null,"id":"3541807d","metadata":{"id":"3541807d"},"outputs":[],"source":["from sklearn.base import BaseEstimator, ClassifierMixin\n","from regsubset import prepare_data\n","\n","class Classifier(BaseEstimator, ClassifierMixin):\n","    def __init__(self, classifier):\n","        self.classifier = classifier\n","\n","    def fit(self, X, y, subset):\n","        X = prepare_data(X, subset)\n","        self.fitted_ = self.classifier.fit(X, y)\n","        return self\n","\n","    def predict(self, X, subset):\n","        X = prepare_data(X, subset)\n","        return self.fitted_.predict(X)\n","\n","    def evaluate(self, X, y_true, subset):\n","        y_pred = self.predict(X, subset)\n","        accuracy = ..\n","        metrics = {..}\n","        return score, metrics"]},{"cell_type":"markdown","id":"bf0fddaf","metadata":{"id":"bf0fddaf"},"source":["2. What is the best value for `n_neighbors` for a best 5-feature model?"]},{"cell_type":"markdown","id":"36c517b2","metadata":{"id":"36c517b2"},"source":["3. What happens when you supply your wrapped classifier to the `forward_search()` and `backward_search()` functions?"]},{"cell_type":"markdown","id":"8bad596f","metadata":{"id":"8bad596f"},"source":["## 6.5.2 Ridge Regression and the Lasso\n","We will use the `sklearn` package in order to perform ridge regression and the lasso. We will now perform ridge regression and the lasso in order to predict `Salary` on the `Hitters` data. Before proceeding ensure that the missing values have been removed from the data, as described in Section 6.5.1."]},{"cell_type":"code","execution_count":null,"id":"44e03225","metadata":{"id":"44e03225"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV\n","from sklearn.metrics import mean_squared_error\n","\n","import statsmodels.api as sm"]},{"cell_type":"markdown","id":"e4aeaf93","metadata":{"id":"e4aeaf93"},"source":["We reload the data to make sure we start with a clean slate:"]},{"cell_type":"code","execution_count":null,"id":"b04ca654","metadata":{"id":"b04ca654"},"outputs":[],"source":["Hitters = pd.read_csv('./data/islr_data/Hitters.csv')\n","Hitters = Hitters.dropna()\n","Hitters = pd.get_dummies(Hitters, columns=['League', 'Division', 'NewLeague'])\n","Hitters = Hitters.drop('League_A', axis=1).drop('Division_E', axis=1).drop('NewLeague_A', axis=1)\n","y = Hitters['Salary']\n","X = Hitters.drop('Salary', axis=1)"]},{"cell_type":"markdown","id":"bc2db818","metadata":{"id":"bc2db818"},"source":["### Ridge regression\n","By default the `Ridge()` function performs ridge regression for an automatically selected $\\lambda$ value of 1. However, here we have chosen to implement the function over a grid of values ranging from $\\lambda=10^{6}$ to $\\lambda=10^{-3}$, essentially covering the full range of scenarios from the null model containing only the intercept, to the least squares fit. As we will see, we can also compute model fits for a particular value of $\\lambda$ that is not one of the original `grid` values."]},{"cell_type":"code","execution_count":null,"id":"9270d8dd","metadata":{"id":"9270d8dd"},"outputs":[],"source":["lambdas = 10**np.linspace(6,-3,100)"]},{"cell_type":"markdown","id":"14fe5579","metadata":{"id":"14fe5579"},"source":["Associated with each value of $\\lambda$ is a vector of ridge regression coefficients, stored in a numpy array that can be accessed by `.coef_`, as well as the intercept that can be accessed by `.intercept_`. The code below loops through the lambdas and outputs a $19 \\times 100$\n","array, with $19$ rows (one for each predictor) and $100$ columns (one for each value of $\\lambda$) as well as a $1 \\times 100$ array of intercepts. First, we scale the data such that all variables are on the same scale, which is relevant if we want to penalize all weights in the same way."]},{"cell_type":"code","execution_count":null,"id":"62d6d9f8","metadata":{"id":"62d6d9f8"},"outputs":[],"source":["# Standardize variables\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)"]},{"cell_type":"code","execution_count":null,"id":"b1dc81b0","metadata":{"id":"b1dc81b0"},"outputs":[],"source":["ridge = Ridge(fit_intercept=True)\n","\n","# Initialize lists to store coefficients and intercepts\n","coefs, intercepts = [], []\n","\n","for l in range(len(lambdas)):\n","    ridge.set_params(alpha = lambdas[l])    # lambda is confusingly called alpha in scikit-learn\n","    ridge.fit(X_scaled, y)\n","    coefs.append(ridge.coef_)\n","    intercepts.append(ridge.intercept_)\n","\n","print(len(coefs), len(coefs[0]), len(intercepts))"]},{"cell_type":"markdown","id":"XTdxpVy7RvEs","metadata":{"id":"XTdxpVy7RvEs"},"source":["We can now easily plot the weights as a function of $\\lambda$, similar to Figure 6.4(a) of ISLR:"]},{"cell_type":"code","execution_count":null,"id":"32b28a96","metadata":{"id":"32b28a96"},"outputs":[],"source":["ax = plt.gca()\n","ax.plot(lambdas, coefs)\n","ax.set_xscale('log')\n","\n","plt.axis('tight')\n","plt.xlabel('$\\lambda$')\n","plt.ylabel('weights')\n","plt.title('Ridge coefficients as a function of the regularization');"]},{"cell_type":"markdown","id":"2ca2c6a6","metadata":{"id":"2ca2c6a6"},"source":["The figure shows that the coefficient estimates are much smaller, in terms of $\\ell_2$ norm, when a large value of $\\lambda$ is used, as compared to when a small value of\n","$\\lambda$ is used.\n","\n","**Exercise**\n"," - Use the code below to list the coefficients when choosing the $50^{th}$ value of $\\lambda$, 35.1, along with their $\\ell_2$ norm.\n"]},{"cell_type":"code","execution_count":null,"id":"e5270695","metadata":{"id":"e5270695"},"outputs":[],"source":["#ToDo\n","ridge.set_params(alpha = ...?)\n","ridge.fit(X_scaled, y)\n","print(\"Coefficients and intercept: \\n\", ridge.coef_, ridge.intercept_)\n","print(\"L2 norm: \\n\", np.sqrt(np.sum(ridge.coef_**2)))"]},{"cell_type":"markdown","id":"47faa34a","metadata":{"id":"47faa34a"},"source":[" - Now chosing the $60^{th}$ value of $\\lambda$, 4.3, along with their $\\ell_2$ norm. Note the higher $\\ell_2$ norm of the coefficients associated with this smaller value of $\\lambda$."]},{"cell_type":"code","execution_count":null,"id":"4e76f052","metadata":{"id":"4e76f052"},"outputs":[],"source":["#ToDo\n","ridge.set_params(alpha = ...?)\n","ridge.fit(X_scaled, y)\n","print(\"Coefficients {} and intercept: {}\\n\".format(ridge.coef_, ridge.intercept_))\n","print(\"L2 norm: \", np.sqrt(np.sum(ridge.coef_**2)))"]},{"cell_type":"markdown","id":"411cbb48","metadata":{"id":"411cbb48"},"source":["We  now split the samples into a training set and a test set in order to estimate the test error of ridge regression and the lasso using the `train_test_split` function from `sklearn`.\n","\n","We set a random state so that the results obtained will be reproducible."]},{"cell_type":"code","execution_count":null,"id":"6890f8f4","metadata":{"id":"6890f8f4"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.5, random_state=42)"]},{"cell_type":"markdown","id":"2a533427","metadata":{"id":"2a533427"},"source":["Next we fit a ridge regression model on the training set, and evaluate its MSE on the test set, using $\\lambda=1$.  Note the use of the `predict()` function to get predictions for a test set."]},{"cell_type":"code","execution_count":null,"id":"3f09dec6","metadata":{"id":"3f09dec6"},"outputs":[],"source":["ridge = Ridge(alpha = 1)\n","ridge.fit(X_train, y_train)                      # Fit a ridge regression on the training data\n","pred = ridge.predict(X_test)                     # Use this model to predict the test data\n","print(pd.Series(ridge.coef_, index = X.columns)) # Print coefficients\n","print(mean_squared_error(y_test, pred))          # Calculate the test MSE"]},{"cell_type":"markdown","id":"cdee8bc2","metadata":{"id":"cdee8bc2"},"source":["The test MSE is approximately 139,000.\n","Note that if we had instead simply fit a model with just an intercept, we would have predicted each test observation using the mean of the training observations. In that case, we could compute the test set MSE like this:"]},{"cell_type":"code","execution_count":null,"id":"6f542f1b","metadata":{"id":"6f542f1b"},"outputs":[],"source":["np.mean((np.mean(y_train)-y_test)**2)"]},{"cell_type":"markdown","id":"7f5b3916","metadata":{"id":"7f5b3916"},"source":["We could get (nearly) the same result by fitting a ridge regression model with a *very* large value of $\\lambda$.\n","\n","**Exercise**\n"," - Use `1e50`, which means $10^{50}$, as $\\lambda$ and see how the coefficients are all very close to 0."]},{"cell_type":"code","execution_count":null,"id":"395136b0","metadata":{"id":"395136b0"},"outputs":[],"source":["#ToDo\n","ridge.set_params(alpha = ...?)\n","ridge.fit(X_train, y_train)\n","pred = ridge.predict(X_test)\n","print(\"Coefficients {} and intercept: {}\\n\".format(ridge.coef_, ridge.intercept_))\n","\n","print(\"MSE: \\n\", mean_squared_error(y_test, pred))"]},{"cell_type":"markdown","id":"69bfeaa7","metadata":{"id":"69bfeaa7"},"source":["So fitting a ridge regression model with $\\lambda=1$ leads to a much lower test MSE than fitting a model with just an intercept.\n","We now check whether there is any benefit to performing ridge regression with $\\lambda=1$ instead of just performing least squares regression (recall that least squares is simply ridge regression with $\\lambda=0$). Is this the case?"]},{"cell_type":"code","execution_count":null,"id":"090bd0de","metadata":{"id":"090bd0de"},"outputs":[],"source":["sX_train = sm.add_constant(X_train)\n","sX_test = sm.add_constant(X_test)\n","linear = sm.OLS(y_train,sX_train).fit()\n","print(linear.params)\n","pred = linear.predict(sX_test)\n","print(mean_squared_error(y_test, pred))          # Calculate the test MSE"]},{"cell_type":"markdown","id":"63448251","metadata":{"id":"63448251"},"source":["In general, instead of arbitrarily choosing $\\lambda=1$, it would be better to use cross-validation to choose the tuning parameter $\\lambda$.\n","We can do this using the built-in cross-validation function, `RidgeCV()`.  By default, the function performs ten-fold cross-validation, though this can be changed using the argument `cv`. Note that we set a random seed first so our results will be reproducible, since the choice of the cross-validation folds is random."]},{"cell_type":"code","execution_count":null,"id":"d9cec79d","metadata":{"id":"d9cec79d"},"outputs":[],"source":["np.random.seed(42)\n","ridgecv = RidgeCV(alphas=lambdas, cv=10, scoring=\"neg_mean_squared_error\")"]},{"cell_type":"code","execution_count":null,"id":"41b99982","metadata":{"id":"41b99982"},"outputs":[],"source":["ridgecv.fit(X_train, y_train)\n","print(ridgecv.alpha_)\n","\n","# Lambda with best CV performance\n","print(\"CV MSE on training set\", -ridgecv.best_score_)\n","print(\"MSE on test set\", mean_squared_error(ridgecv.predict(X_test),y_test))"]},{"cell_type":"markdown","id":"08b6791b","metadata":{"id":"08b6791b"},"source":["Apparently, a value of 100 for $\\lambda$ gives best cross-validation results here, but a higher test MSE than attained before for $\\lambda = 1$. Alternatively you can use the `cross_val_score()` function in combination with the `ridge()` function. Althought at first slightly more inconvenient, this allows us to access the cross-validation scores for each value of `alpha`:"]},{"cell_type":"code","execution_count":null,"id":"a4a5f89e","metadata":{"id":"a4a5f89e"},"outputs":[],"source":["from sklearn.model_selection import cross_val_score\n","\n","mean_scores = np.zeros(len(lambdas))\n","std_scores = np.zeros(len(lambdas))\n","\n","for i, lambda_ in enumerate(lambdas):\n","    cv = cross_val_score(\n","        Ridge(lambda_), X_train, y_train, cv=10, scoring='neg_mean_squared_error'\n","    )\n","    mean_scores[i] = cv.mean()\n","    std_scores[i] = cv.std()\n","\n","print(\"Minimum CV MSE of {} at lambda: {} (index {})\".format(\n","    np.min(-mean_scores),\n","    lambdas[np.argmin(-mean_scores)],\n","    np.argmin(-mean_scores)\n","))\n","\n","plt.errorbar(x=lambdas, y=-mean_scores, yerr=std_scores)\n","mn, opt = np.min(-mean_scores), np.argmin(-mean_scores)\n","plt.scatter(lambdas[opt],mn,c='red')\n","plt.xscale(\"log\")\n","plt.xlabel('$\\lambda$')\n","plt.ylabel('CV MSE')\n","plt.show()"]},{"cell_type":"markdown","id":"4829b615","metadata":{"id":"4829b615"},"source":["**Exercise**\n"," - Find the value of $\\lambda$ chosen by cross-validation (Check the $mean\\_scores$ variable) refit our ridge regression model on the full data set and examine the coefficient estimates."]},{"cell_type":"code","execution_count":null,"id":"7566da62","metadata":{"id":"7566da62"},"outputs":[],"source":["#ToDo\n","ridge.set_params(alpha=...?)\n","ridge_fit = ridge.fit(X_scaled, y)\n","print(pd.Series(ridge_fit.coef_, index=X.columns))"]},{"cell_type":"markdown","id":"0e4a7820","metadata":{"id":"0e4a7820"},"source":["As expected, none of the coefficients are zero --- ridge regression does not perform variable selection!\n"]},{"cell_type":"markdown","id":"2025d6b0","metadata":{"id":"2025d6b0"},"source":["### The Lasso\n","We saw that ridge regression with a wise choice of $\\lambda$ can outperform least squares as well as the null model on the `Hitters` data set. We now ask whether the lasso can yield either\n","a more accurate or a more interpretable model than ridge regression. In order to fit a lasso model, we  use the `Lasso()` function:"]},{"cell_type":"code","execution_count":null,"id":"83806eb9","metadata":{"id":"83806eb9"},"outputs":[],"source":["lasso = Lasso(max_iter=10000) # Setting max_iter prevents ConvergenceWarning\n","\n","# Try 100 lambdas ranging from 10000 down to 0.01\n","lambdas = 10 ** np.linspace(6,-3,100)\n","\n","# Initialize lists to store coefficients and intercepts\n","coefs, intercepts = [], []\n","\n","for l in range(len(lambdas)):\n","    lasso.set_params(alpha=lambdas[l])\n","    lasso.fit(X_scaled, y)\n","    coefs.append(lasso.coef_)\n","    intercepts.append(lasso.intercept_)"]},{"cell_type":"code","execution_count":null,"id":"bc3e5e4f","metadata":{"id":"bc3e5e4f"},"outputs":[],"source":["ax = plt.gca()\n","ax.plot(lambdas, coefs)\n","ax.set_xscale('log')\n","\n","plt.axis('tight')\n","plt.xlabel('alpha')\n","plt.ylabel('weights')\n","plt.title('Lasso coefficients as a function of the regularization');"]},{"cell_type":"markdown","id":"3792c411","metadata":{"id":"3792c411"},"source":["We can see from the coefficient plot that depending on the choice of tuning parameter, some of the coefficients will be exactly equal to zero.\n","We now perform cross-validation and compute the associated test error."]},{"cell_type":"code","execution_count":null,"id":"63dba029","metadata":{"id":"63dba029"},"outputs":[],"source":["mean_scores = np.zeros(len(lambdas))\n","std_scores = np.zeros(len(lambdas))\n","\n","for i, lambda_ in enumerate(lambdas):\n","    cv = cross_val_score(\n","      Lasso(lambda_, max_iter=10000),\n","      X_train, y_train, cv=10, scoring='neg_mean_squared_error'\n","    )\n","    mean_scores[i] = cv.mean()\n","    std_scores[i] = cv.std()\n","\n","print(\"Minimum CV MSE of {} at lambda: {} (index {})\".format(\n","    np.min(-mean_scores),\n","    lambdas[np.argmin(-mean_scores)],\n","    np.argmin(-mean_scores)\n","))\n","\n","plt.errorbar(x=lambdas, y=-mean_scores, yerr=std_scores)\n","mn, opt = np.min(-mean_scores), np.argmin(-mean_scores)\n","plt.scatter(lambdas[opt],mn,c='red')\n","plt.xscale(\"log\")\n","plt.xlabel('$\\lambda$')\n","plt.ylabel('CV MSE')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"61914ea4","metadata":{"id":"61914ea4"},"outputs":[],"source":["lasso.set_params(alpha=lambdas[np.argmin(-mean_scores)])\n","lasso.fit(X_train, y_train)\n","print('MSE on test set:', mean_squared_error(y_test, lasso.predict(X_test)))"]},{"cell_type":"markdown","id":"5743a1cc","metadata":{"id":"5743a1cc"},"source":["This is not the best MSE obtained thus far, but lower than the test MSE of ridge regression with $\\lambda$ optimized by cross-validation.\n","\n","In general, the lasso has an advantage over ridge regression in that the resulting coefficient estimates are sparse. Here we see that 10 of the 19 coefficient estimates are set to zero:"]},{"cell_type":"code","execution_count":null,"id":"a488c7b5","metadata":{"id":"a488c7b5"},"outputs":[],"source":["print(pd.Series(lasso.coef_, index=X.columns))"]},{"cell_type":"markdown","id":"KJRWFtQNmLze","metadata":{"id":"KJRWFtQNmLze"},"source":["This concludes the lab session, you should now have seen all the code you need to perform the applied exercises."]},{"cell_type":"markdown","id":"d024b9d4","metadata":{"id":"d024b9d4"},"source":["# References\n","\n","This notebook is based on R notebooks provided by the authors of ISLR and on the one found for Chapter 6 of ISLR in https://github.com/JWarmenhoven/ISLR-python."]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["2439aa84","8bad596f"]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}